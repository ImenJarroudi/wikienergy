{
 "metadata": {
  "name": "",
  "signature": "sha256:5bd4797360199d45e5bce53eb6727b0063b3debb04886e38e540d263b2274ff0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import sys\n",
      "import os.path\n",
      "sys.path.append(os.path.abspath(os.path.join(os.pardir,os.pardir)))\n",
      "import disaggregator as da\n",
      "import disaggregator.PecanStreetDatasetAdapter as psda\n",
      "import pickle\n",
      "import numpy as np\n",
      "import pdb\n",
      "import matplotlib.pyplot as plt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dataset Creation\n",
      "================\n",
      "\n",
      "This notebook focuses on creating machine-learning datasets amenable to use for appliance detection. It builds upon the `PecanStreetDatasetAdpater` functions and may integrate its results into those functions.\n",
      "\n",
      "Goals\n",
      "-----\n",
      "This notebook\n",
      "- develops an interface for direct creation of usable datasets.\n",
      "- focuses on case study of EV detection.\n",
      "- focuses on the shared dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_url = \"postgresql://USERNAME:PASSWORD@db.wiki-energy.org:5432/postgres\"\n",
      "psda.set_url(db_url)\n",
      "\n",
      "schema = 'shared'\n",
      "tables = [u'validated_01_2014',\n",
      "          u'validated_02_2014',\n",
      "          u'validated_03_2014',\n",
      "          u'validated_04_2014',\n",
      "          u'validated_05_2014',]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get dataids\n",
      "\n",
      "# uncomment the following line to to regenerate this list\n",
      "# common_use_ids = da.utils.get_common_ids([psda.get_dataids_with_real_values(schema,table,'use') for table in tables])\n",
      "\n",
      "common_use_ids = [86, 93, 94, 410, 484, 585, 624, 661, 739, 744, 821, 871, 936, 1167, 1283, 1334, 1632, 1714, 1718, 1782,\n",
      "                  1790, 1800, 1953, 1994, 2094, 2129, 2156, 2158, 2171, 2233, 2242, 2337, 2449, 2470, 2575, 2606, 2638, 2769,\n",
      "                  2814, 2829, 2864, 2945, 2953, 2974, 3092, 3192, 3221, 3263, 3367, 3394, 3456, 3482, 3504, 3544, 3649, 3652,\n",
      "                  3723, 3736, 3778, 3795, 3893, 3918, 4031, 4135, 4154, 4298, 4313, 4447, 4505, 4526, 4641, 4732, 4767, 4874,\n",
      "                  4922, 4956, 4957, 4998, 5026, 5109, 5209, 5218, 5262, 5275, 5357, 5395, 5545, 5568, 5677, 5785, 5814, 5874,\n",
      "                  5938, 5949, 5972, 6139, 6412, 6636, 6673, 6730, 6836, 6910, 6941, 7062, 7319, 7390, 7531, 7536, 7617, 7731,\n",
      "                  7769, 7788, 7800, 7850, 7863, 7875, 7940, 7951, 8046, 8079, 8084, 8142, 8197, 8292, 8317, 8342, 8419, 8467,\n",
      "                  8645, 8669, 8741, 8829, 8852, 8956, 9019, 9036, 9121, 9160, 9343, 9356, 9484, 9555, 9578, 9609, 9643, 9654,\n",
      "                  9701, 9729, 9737, 9771, 9830, 9875, 9915, 9922, 9926, 9932, 9934, 9937, 9938, 9939, 9982, 9983]\n",
      "common_car_ids = [624, 661, 1714, 1782, 1953, 2470, 2638, 2769, 2814, 3192, 3367, 3482, 3723, 3795, 4135, 4505, 4526, 4641,\n",
      "                   4767, 4957, 4998, 5109, 5357, 6139, 6836, 6910, 6941, 7850, 7863, 7875, 7940, 8046, 8142, 8197, 8645, 8669,\n",
      "                   9484, 9609, 9729, 9830, 9932, 9934]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Description of an ideal dataset:***\n",
      "\n",
      "The ideal dataset consists of as much differentiating information as possible about what it looks like for a device to be present. The dataset should be a big numpy array of data and a big numpy array of labels.\n",
      "\n",
      "***Description of some useful functions***\n",
      "\n",
      "Function that takes an appliance name and a dataid and gets all windows of data in the available months."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_use_for_active_windows(schema, tables, appliances, dataids,\n",
      "                               window_length, window_stride,\n",
      "                               drop_percentile=10, sample_rate='15T'):\n",
      "    '''\n",
      "    Given a list of consecutive tables, returns windows of total use data\n",
      "    for which the appliance waas active. Appliances should not include the\n",
      "    'use' column. Drops the lowest drop_percentile samples. Use \n",
      "    appliances=None for unfiltered windows.\n",
      "    '''\n",
      "    if not appliances:\n",
      "        appliances = []\n",
      "    appliances.append('use')\n",
      "    instances = psda.generate_instances_for_appliances_by_dataids(\n",
      "        schema,tables,appliances,dataids,sample_rate)\n",
      "    usages = [instances_for_id[-1] for instances_for_id in instances]\n",
      "    instances = [instances_for_id[:-1] for instances_for_id in instances]\n",
      "    all_appliance_windows = []\n",
      "    for usage, instances_ in zip(usages,instances): # iterate over dataids\n",
      "        assert(len(usage.traces) == 1)\n",
      "        usage_windows = da.utils.get_trace_windows(usage.traces[0],window_length,window_stride)\n",
      "        appliance_windows = []\n",
      "        if instances.empty():\n",
      "            if drop_percentile is not 0:\n",
      "                print \"Warning: ignoring drop_percentile\"\n",
      "            \n",
      "            # remove nans\n",
      "            windows = np.nan_to_num(np.array(usage_windows))\n",
      "\n",
      "            appliance_windows.append(windows)\n",
      "        else:\n",
      "            for instance in instances_: # iterate over appliances\n",
      "                assert(len(instance.traces) == 1)\n",
      "                appliance_window_array = da.utils.get_trace_windows(instance.traces[0],window_length,window_stride)\n",
      "                window_totals = np.sum(appliance_window_array,axis = 1)\n",
      "\n",
      "                # drop usage windows for which the appliance totals are in the bottom few percentiles\n",
      "                n_keep = window_totals.shape[0]*(100-drop_percentile)/100\n",
      "                keep_indices = sorted(np.argsort(window_totals)[::-1][:n_keep])\n",
      "\n",
      "                # remove nans\n",
      "                windows = np.nan_to_num(np.array([usage_windows[i] for i in keep_indices]))\n",
      "\n",
      "                appliance_windows.append(windows)\n",
      "        all_appliance_windows.append(appliance_windows)\n",
      "    return all_appliance_windows\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_appliance_detection_dataset(schema,tables,appliance,window_length,window_stride,drop_percentile):\n",
      "    print \"Fetching dataids\"\n",
      "    all_ids = da.utils.get_common_ids([psda.get_dataids_with_real_values(schema,table,'use') for table in tables])\n",
      "    appliance_ids = da.utils.get_common_ids([psda.get_dataids_with_real_values(schema,table,appliance) for table in tables])\n",
      "    no_appliance_ids = sorted(list(set(all_ids) - set(appliance_ids)))\n",
      "    \n",
      "    # generate random dataid indices\n",
      "    app_i_train, app_i_valid, app_i_test = da.utils.get_train_valid_test_indices(len(appliance_ids))\n",
      "    no_app_i_train, no_app_i_valid, no_app_i_test = da.utils.get_train_valid_test_indices(len(no_appliance_ids))\n",
      "    \n",
      "    set_indices = [(\"Training\",app_i_train,no_app_i_train),\n",
      "                   (\"Validation\",app_i_valid,no_app_i_valid),\n",
      "                   (\"Testing\",app_i_train,no_app_i_test)]\n",
      "    \n",
      "    training_sets = []\n",
      "    \n",
      "    for set_name,app_i,no_app_i in set_indices:\n",
      "    \n",
      "        print set_name\n",
      "        # get dataids\n",
      "        appliance_ids = [appliance_ids[i] for i in app_i]\n",
      "        no_appliance_ids = [no_appliance_ids[i] for i in no_app_i]\n",
      "\n",
      "        # get windows for each dataid\n",
      "        print \"Fetching samples for key = 1\"\n",
      "        active_appliance_windows = get_use_for_active_windows(schema,tables,[appliance],appliance_ids,window_length,window_stride,drop_percentile)\n",
      "        print \"Fetching smaples for key = 0\"\n",
      "        other_windows = get_use_for_active_windows(schema,tables,None,no_appliance_ids,window_length,window_stride,drop_percentile=0)\n",
      "\n",
      "        # concatenate results for different dataids\n",
      "        all_appliance_windows = np.concatenate([windows[0] for windows in active_appliance_windows],axis=0)\n",
      "        all_other_windows = np.concatenate([windows[0] for windows in other_windows],axis=0)\n",
      "\n",
      "        # make one-hot answer-key arrays\n",
      "        appliance_keys = np.array([[0,1] for _ in all_appliance_windows])\n",
      "        no_appliance_keys = np.array([[1,0] for _ in all_other_windows])\n",
      "\n",
      "        # concatenate all training examples\n",
      "        X = np.concatenate([all_appliance_windows_train,all_other_windows_train],axis=0)\n",
      "        y = np.concatenate([appliance_keys_train,no_appliance_keys_train],axis=0)\n",
      "\n",
      "        training_sets.append(ds.DenseDesignMatrix(X=X,y=y))\n",
      "    train, valid, test = training_sets\n",
      "    return train,valid,test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train,valid,test = get_appliance_detection_dataset(schema,tables,'car1',672,96,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}